# web_scrape_freelance
upwork proposal and working code web scrapping real estate parcel data see example 250 row csv





 -SUMMARY-
 THIS WAS FOR AN UPWORK PROPOSAL WHERE THE CLIENT WANTED TO SCRAPE REAL ESTATE PARCEL INFORMATION BY COUNTY, THIS COUNTY HAD  A PRETTY OPEN 
WEBSITE WHICH ALLOW FOR SEARCHING WITH A WILDCARD CHARACTER TO RETURN ALL PARCEL RESULTS, YET THEY PLACED LIMITATIONS ON  PAGE VIEW PER MINUTE (15) AND PER TOKEN USAGE WHICH I ESTIMATE WAS 150 - 300 PAGE VIEWS, PLUS  HUMAN OR BOT VERIFICATION WALLS.  ULTIMATELY THIS JOB WAS UNFEASIBLE FOR ONE COMPUTER FOR COUNTIES ANY LARGER AS THEY WOULD HAVE 250K ENTRIES AT 15 PAGE VIEW PER MINUTE WOULD HAVE TAKEN MORE TIME THAN POSSIBLE WITHOUT A MULTI BOT APPROACH OFFERED BY LARGER WEB SCRAPE COMPANIES. 

-ABOUT THE CODE-
Tech :  selenium , splinter , BeautifulSoup, Python 
 IT TOOK ME ABOUT [ 27 HOURS  OR 1.2 WEEKS PART TIME] TO COMPLETE THE 2 PYTHON SCRIPTS. THERE WAS AN UNCONVENTIONAL LAYOUT TO EACH HTML PAGE. THE WEB DEVELOPER MADE SOME INTERESTING CHOICES IN POPULATING CONTENT, WHICH LED TO MANY UNIQUE ALGORITHMS TO FIND, AND UNPACK THE DATA, IN THESE CASES BEAUTIFUL SOUP WAS ABLE TO GET ME CLOSE BUT THEN CUSTOM ALGORITHMS WERE NEEDED TO GO ALL THE WAY! DUE TO NATURE OF THE WORK I DESIGNED THE SCRAPPING FUNCTION IN A WHILE LOOP EXECUTE BETWEEN 2 PYTHON SCRIPTS. THIS WAS MY  SECOND WEB SCRAPE PROJECT EVER, (FIRST WAS IN BOOTCAMP) AND I WAS QUICKLY ABLE TO GET UP AND RUNNING ( BY REFERRING TO PRIOR WORK EXAMPLES), AND TO START GETTING DATA. I FOUND THE PROJECT TO BE AN ENJOYABLE PUZZLE TO FIND AND UNLOCK ALL THE DATA THAT WAS AVAILABLE, WHICH COULD BE UP TO 200 UNIQUE COLUMNS/DATA POINTS PER PAGE. 

SEE INCLUDED PYTHON SCRIPTS AND  CSV EXAMPLE   'c_county_parcel_data_first250_rows.csv'